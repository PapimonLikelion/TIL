### 2022-09-27

## WebClient 에러 로그
```
19:54:04.891 [ERROR] [reactor-http-epoll-4] [reactor.core.publisher.Operators] - Operator called default onErrorDropped
reactor.core.Exceptions$ErrorCallbackNotImplemented: org.springframework.web.reactive.function.client.WebClientRequestException: readAddress(..) failed: Connection reset by peer; 
nested exception is io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
Caused by: org.springframework.web.reactive.function.client.WebClientRequestException: readAddress(..) failed: Connection reset by peer; 
nested exception is io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
	at org.springframework.web.reactive.function.client.ExchangeFunctions$DefaultExchangeFunction.lambda$wrapException$9(ExchangeFunctions.java:141)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ Request to GET https://kip17-api.klaytnapi.com/v2/contract/0xe99540401ef24aba1b7076ea92c94ec38536c6fb/owner/0x3ab31d219d45ce40d6862d68d37de6bb73e21a8d [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.ExchangeFunctions$DefaultExchangeFunction.lambda$wrapException$9(ExchangeFunctions.java:141)
		at reactor.core.publisher.MonoErrorSupplied.subscribe(MonoErrorSupplied.java:55)
		at reactor.core.publisher.Mono.subscribe(Mono.java:4397)
		at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onError(FluxPeek.java:222)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onError(FluxPeek.java:222)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onError(FluxPeek.java:222)
		at reactor.core.publisher.MonoNext$NextSubscriber.onError(MonoNext.java:93)
		at reactor.core.publisher.MonoFlatMapMany$FlatMapManyMain.onError(MonoFlatMapMany.java:204)
		at reactor.core.publisher.SerializedSubscriber.onError(SerializedSubscriber.java:124)
		at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.whenError(FluxRetryWhen.java:225)
		at reactor.core.publisher.FluxRetryWhen$RetryWhenOtherSubscriber.onError(FluxRetryWhen.java:274)
		at reactor.core.publisher.FluxConcatMap$ConcatMapImmediate.drain(FluxConcatMap.java:415)
		at reactor.core.publisher.FluxConcatMap$ConcatMapImmediate.onNext(FluxConcatMap.java:251)
		at reactor.core.publisher.EmitterProcessor.drain(EmitterProcessor.java:537)
		at reactor.core.publisher.EmitterProcessor.tryEmitNext(EmitterProcessor.java:343)
		at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
		at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
		at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:190)
		at reactor.core.publisher.MonoCreate$DefaultMonoSink.error(MonoCreate.java:201)
		at reactor.netty.http.client.HttpClientConnect$HttpObserver.onUncaughtException(HttpClientConnect.java:400)
		at reactor.netty.ReactorNetty$CompositeConnectionObserver.onUncaughtException(ReactorNetty.java:670)
		at reactor.netty.resources.DefaultPooledConnectionProvider$DisposableAcquire.onUncaughtException(DefaultPooledConnectionProvider.java:205)
		at reactor.netty.resources.DefaultPooledConnectionProvider$PooledConnection.onUncaughtException(DefaultPooledConnectionProvider.java:454)
		at reactor.netty.channel.FluxReceive.drainReceiver(FluxReceive.java:232)
		at reactor.netty.channel.FluxReceive.onInboundError(FluxReceive.java:453)
		at reactor.netty.channel.ChannelOperations.onInboundError(ChannelOperations.java:488)
		at reactor.netty.channel.ChannelOperationsHandler.exceptionCaught(ChannelOperationsHandler.java:126)
		at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302)
		at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:273)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireExceptionCaught(CombinedChannelDuplexHandler.java:424)
		at io.netty.channel.ChannelHandlerAdapter.exceptionCaught(ChannelHandlerAdapter.java:92)
		at io.netty.channel.CombinedChannelDuplexHandler$1.fireExceptionCaught(CombinedChannelDuplexHandler.java:145)
		at io.netty.channel.ChannelInboundHandlerAdapter.exceptionCaught(ChannelInboundHandlerAdapter.java:143)
		at io.netty.channel.CombinedChannelDuplexHandler.exceptionCaught(CombinedChannelDuplexHandler.java:231)
		at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302)
		at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:273)
		at io.netty.handler.ssl.SslHandler.exceptionCaught(SslHandler.java:1105)
		at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302)
		at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:273)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.exceptionCaught(DefaultChannelPipeline.java:1377)
		at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:302)
		at io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:281)
		at io.netty.channel.DefaultChannelPipeline.fireExceptionCaught(DefaultChannelPipeline.java:907)
		at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.handleReadException(AbstractEpollStreamChannel.java:728)
		at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:826)
		at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:487)
		at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:385)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
```
- Connection Provider : https://github.com/reactor/reactor-netty/issues/1774

## 유의미한 에러 로그
- **Operator called default onErrorDropped**
  - *참고: https://tacogrammer.com/onerrordropped-explained/*
  - 높은 동시성을 처리하려고 리액티브 쓰는건데... 비동기로 여러 장소에서 데이터 수신하면 해당 에러 발생할 수도
  - 이미 **다른 스레드에서 onError가 발생**해 **전체 스트림이 terminated**된 상황인 경우

- **The connection observed an error, the request cannot be retried as the headers/body were sent
  io.netty.channel.unix.Errors$NativeIoException: 
  readAddress(..) failed: Connection reset by peer**
  - *참고: https://stackoverflow.com/questions/55233216/spring-webflux-webclient-logs-connection-reset-by-peer*
    - 문제: Client goes away after connecting to it once and the next request fails - then retries
      - 클라이언트가 한번 커넥트 시도하고 그다음 리퀘스트가 fail 한 경우 사라짐
    - 해결: Connection pooling을 없애보는건 어떠니? 
     ```java
     @Bean
     public WebClient webClient() {
         return WebClient.builder()
                  .clientConnector(connector())
                  .build();
     }

     private ClientHttpConnector connector() {
         return new ReactorClientHttpConnector(HttpClient.from(TcpClient.newConnection()));
     }
     ```
  - *참고: https://github.com/reactor/reactor-netty/issues/388*
    - 약간 고질적인 문제같은데...?

## Connection reset by Peer
- *참고: https://groups.google.com/g/vertx/c/3o_DEwIK9dY*
- 커넥션이 다른 쪽에서 끊어진 경우 호출됨
  - 타 웹서버에서 connection을 끊은경우 호출됨
  - 타 웹서버는 idle connection을 몇초 후에 끊어버리는것이 일반적

## Keep-Alive
- *참고: https://developer.mozilla.org/ko/docs/Web/HTTP/Headers/Keep-Alive*
- *참고: https://etloveguitar.tistory.com/137*
- *참고: https://goodgid.github.io/HTTP-Keep-Alive/*
- `Keep-Alive: timeout=x, max=y`
  - 각각 등호로 구분되는 식별자와 값으로 구성됨
  - timeout: 유휴 연결이 계속 열려있어야 되는 최소한의 시간
  - max: 연결이 닫히기 이전에 전송될 수 있는 최대 요청 수
  - HTTP/1.0+ 부터 지원

- **Persistent Connection**
  - Persistent Connection: 요청 처리 이후에도 connection 유지하는 것
    - site locality: 서버에 연속적으로 동일한 클라이언트가 여러 요청을 보낼 가능성이 높은 경우
    - HTTP/1.1 부터 HTTP 어플리케이션이 TCP Connection을 요청마다 close하지 않고 재사용할 수 있는 방법 제공
  - 왜 필요하지?
    - TCP 연결 맺기 위해 SYN-ACK 3-way handshake 매 요청마다 맺을 필요가 없어짐
      1. 네트워크 혼잡 감소: TCP, SSL/TCP connection request 수 줄어듦
      2. 네트워크 비용 감소: 여러개의 connection으로 하나의 client 요청 serving하는 것 보다는 한개의 client 요청 서빙하는게 효율적
      3. latency 감소: 3-way-handshake round-trip 줄어듦

- **Keep-Alive 옵션 규칙들**
  - 클라이언트 측에서 모든 요청에 위에 언급한 헤더를 담아 보내야 함
    - 그게 아니라면 서버는 연결을 close 함
  - 서버 또한 persistent 하게 요청 주고받다가 response에 keep-alive 관련 헤더 안 담겨오면 클라이언트 측에서 서버가 persistent connection 맺고 있지 않다고 판단
  - 정확한 Content-Length 사용할 것
  - **Connection 헤더를 지원하지 않는 Proxy에서는 사용할 수 없음**
  - 클라이언트는 언제든 connection close 될 수 있으니 retry 로직 준비해 둘 것

- **Keep-Alive & 멍청한 Proxy**
  - ![](../images/2022-09-27-keep-alive-proxy.png)
  - Proxy : 전 Keep-Alive 몰라요!
  - Client <-> Proxy <-> Server
    - Client -> Proxy : Keep-Alive
      - Proxy -> Server : Keep-Alive
        - 서버 입장 : "어 Proxy"가 Keep-Alive 지원하네? 커넥션 켜놔야지~
      - Server -> Proxy : Keep-Alive
        - 프록시 입장 : Keep-Alive가 뭐여... 일단 전달. 
          - 삭제하지 않고 요청 그대로를 전달하는게 문제!
    - Proxy -> Client : Keep-Alive
      - 오케이 연결됐다!
  - Client 동일한 Connection에 request
    - Proxy에서는 이미 끊내버린 Connection => 해당 요청 무시 => handshake 안해서 그런가보다
    - Client 입장에서 다음 요청을 보내기 시작할 때 커넥션이 유지되고 있는 프록시에 요청을 보냄
    - 프록시는 같은 커넥션상에서 다른 요청이 오는 경우는 예상하지 못하기 때문에 해당 요청은 프록시로 부터 무시됨
    - Client는 무한정 대기하다 타임아웃 나서 커넥션이 끊김

## WebClient Default Keep-Alive
- *참고: https://stackoverflow.com/questions/4699013/does-webclient-use-keepalive*
- 독스엔 없지만 일단 그런듯

## KAS Keep-Alive
- ![](../images/2022-09-27-kas-connection.png)
- 일단 KAS는 Connection 관련한 헤더를 응답해주지 않음
  - 요청에 Connection = Keep-Alive, Proxy-Connection=Keep-Alive 보내도 똑같음
- 뭐 프록시를 쓰던, 그냥 서버던 Connection 응답이 없다는 건 Keep-Alive 

## 그럼 KAS 모듈 어떻게 해야지..?
- 우선 매번 HTTP 요청을 단독으로 할 수 있도록 하자. (3-way-handshake 하고 연결 수립)
  - 이렇게 하면 요청에 대한 Connection을 재활용 안하니까 Timeout 이 안날 것 같아
  - Timeout보다만 빠르면 우선 현 상황보단 발전한거니까

## reactor-http-epoll-#
- 이건 쓰레드 풀로 동작하는건가?
- vs `http-nio-8080-exec-#`

## 스프링 쓰레드 풀 with Tomcat

## 왜 WebClient는 netty를 사용하는거지?

## Tomcat vs Netty
